{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate files for training and prediction.\n",
    "# input is WSGA selected columns\n",
    "# remeber to change prefix to HS or HIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import pysam\n",
    "import numpy as np\n",
    "import sys\n",
    "import Bio.SubsMat.MatrixInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefix = '.HIS.'\n",
    "#prefix = '.All.'\n",
    "#prefix = '.HS.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# consider save the file as a dict for future loading\n",
    "FASTA_LOC = '/data/hq2130/large_files/resources/hg19.fasta' # need to modify\n",
    "REVEL = '/data/hq2130/large_files/revel_file/revel_all_chr.txt.gz'  # REVEL loc\n",
    "f_revel= pysam.TabixFile(REVEL)\n",
    "MPC = '/data/hq2130/large_files/fordist_constraint_official_mpc_values.txt.gz'  # mpc\n",
    "f_MPC = pysam.TabixFile(MPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrix_score(a_0, a_1, name_matrix=\"blosum62\"):\n",
    "    \"\"\"\n",
    "    Input: a str a_0, a str a_1, a str name_matrix\n",
    "    Output: The matrix score of a_0 and a_1 in the matrix name_matrix\n",
    "    \"\"\"\n",
    "    # Biopython also included placeholder amino acids (B. J, X, Z) in its\n",
    "    # scoring matrix.\n",
    "\n",
    "    matrix = getattr(Bio.SubsMat.MatrixInfo, name_matrix)\n",
    "\n",
    "    # Since PAM250 in Biopython is not symmetric, if (a_0, a_1) does not exist,\n",
    "    # matrix_score() will check if (a_1, a_0) exists.\n",
    "\n",
    "    if (a_0, a_1) in matrix:\n",
    "        return matrix[(a_0, a_1)]\n",
    "    elif (a_1, a_0) in matrix:\n",
    "        return matrix[(a_1, a_0)]\n",
    "    else:\n",
    "        return -1\n",
    "        #raise KeyError(\"({}, {}) does not exist in matrix.\".format(a_0, a_1))\n",
    "\n",
    "# SCORE = []\n",
    "# aasets = ['G','A','V','L','I','P','S','D','E','N','Q','K','R','H','F','Y','W','M','C','B','Z','X']\n",
    "# for a0 in aasets:\n",
    "#     for a1 in aasets:\n",
    "#         print a0, a1\n",
    "#         print matrix_score(a0,a1, 'blosum62')\n",
    "#         SCORE.append(matrix_score(a0,a1, 'blosum62'))\n",
    "# print np.median(SCORE) #-1\n",
    "# print np.mean(SCORE) #~-1\n",
    "\n",
    "        \n",
    "        \n",
    "## correct GCcontent= (pos~pos+10)/5 to (pos-5~pos+5)/10 by cc on 06/29/2017 \n",
    "def add_gc_content(info):\n",
    "    chrom, pos = info['hg19_chr'], float(info['hg19_pos(1-based)'])\n",
    "    fasta_file = FASTA_LOC\n",
    "    #fasta_file = '/home/local/ARCS/hq2130/Exome_Seq/resources/hg19.fasta' # on server\n",
    "    fastafile = pysam.Fastafile(fasta_file)\n",
    "    seq = fastafile.fetch(chrom, pos - 5, pos + 5).upper()\n",
    "    gc_count = 0\n",
    "    for dna in seq:\n",
    "        if dna in {'G', 'C'}:\n",
    "            gc_count += 1\n",
    "    gc_count = gc_count / 10.0\n",
    "    info['gc_content'] = gc_count\n",
    "    return info\n",
    "\n",
    "def add_s_het(info):\n",
    "    gene = info['genename']\n",
    "    # s_het\n",
    "    info['s_het'] = 0.01876\n",
    "    if gene in s_het:\n",
    "        info['s_het'] = s_het[gene]\n",
    "    # s_het log, default for log transform of 0\n",
    "    info['s_het_log'] = np.log(0.01876) \n",
    "    if gene in s_het:\n",
    "        info['s_het_log'] = np.log(s_het[gene]) # minimal value of s_het = 0.000206342\n",
    "    # info['s_hat_log'] = -10 # default for log transform of 0\n",
    "    # if gene in s_hat:\n",
    "    #     info['s_hat_log'] = np.log(s_hat[gene])\n",
    "    return info\n",
    "\n",
    "\n",
    "# SCORE = []\n",
    "# genesets = []\n",
    "# NA_genenum =0\n",
    "# with open ('../data/gene/genename_list.txt','rb') as fin:\n",
    "#     r = csv.reader(fin)\n",
    "#     for line in r: # line is a list! not a string\n",
    "#         #print line[0]\n",
    "#         genesets.append(line)\n",
    "# for a in genesets:\n",
    "#     if a[0] in s_het.keys(): # a is a single-item list, not hashable, cannot used as dict keys\n",
    "#         SCORE.append(s_het[a[0]])\n",
    "#     else:\n",
    "#         NA_genenum += 1\n",
    "# print np.median(SCORE) #0.0187637535\n",
    "# print np.mean(SCORE) #0.0590276137112\n",
    "# print np.min(SCORE) #0.000206342\n",
    "# print NA_genenum #2121\n",
    "\n",
    "def add_exac_metric(info):\n",
    "    info['pli'] = pli.get(info['genename'], '0.0277110067492')\n",
    "    info['lofz'] = lofz.get(info['genename'], '1.98235565023')\n",
    "    info['prec'] = prec.get(info['genename'], '0.518836940302')\n",
    "    return info\n",
    "\n",
    "# SCORE = []\n",
    "# genesets = []\n",
    "# NA_genenum =0\n",
    "# with open ('../data/gene/genename_list.txt','rb') as fin:\n",
    "#     r = csv.reader(fin)\n",
    "#     for line in r: # line is a list! not a string\n",
    "#         #print line[0]\n",
    "#         genesets.append(line)\n",
    "# for a in genesets:\n",
    "#     if a[0] in pli.keys(): # a is a single-item list, not hashable, cannot used as dict keys\n",
    "#         SCORE.append(pli[a[0]])\n",
    "#     else:\n",
    "#         NA_genenum += 1\n",
    "# print \"pli\"\n",
    "# print np.median(SCORE) #0.0277110067492\n",
    "# print np.mean(SCORE) #0.302933467654\n",
    "# print np.min(SCORE) #5.35738960908e-91\n",
    "\n",
    "# SCORE = []\n",
    "# genesets = []\n",
    "# NA_genenum =0\n",
    "# with open ('../data/gene/genename_list.txt','rb') as fin:\n",
    "#     r = csv.reader(fin)\n",
    "#     for line in r: # line is a list! not a string\n",
    "#         #print line[0]\n",
    "#         genesets.append(line)\n",
    "# for a in genesets:\n",
    "#     if a[0] in lofz.keys(): # a is a single-item list, not hashable, cannot used as dict keys\n",
    "#         SCORE.append(lofz[a[0]])\n",
    "#     else:\n",
    "#         NA_genenum += 1\n",
    "# print \"lofz\"\n",
    "# print np.median(SCORE) #1.98235565023\n",
    "# print np.mean(SCORE) #2.19363750372\n",
    "# print np.min(SCORE) #-9.7770738652\n",
    "\n",
    "# SCORE = []\n",
    "# genesets = []\n",
    "# NA_genenum =0\n",
    "# with open ('../data/gene/genename_list.txt','rb') as fin:\n",
    "#     r = csv.reader(fin)\n",
    "#     for line in r: # line is a list! not a string\n",
    "#         #print line[0]\n",
    "#         genesets.append(line)\n",
    "# for a in genesets:\n",
    "#     if a[0] in prec.keys(): # a is a single-item list, not hashable, cannot used as dict keys\n",
    "#         SCORE.append(prec[a[0]])\n",
    "#     else:\n",
    "#         NA_genenum += 1\n",
    "# print \"prec\"\n",
    "# print np.median(SCORE) #0.518836940302\n",
    "# print np.mean(SCORE) #0.49179236685\n",
    "# print np.min(SCORE) #6.80216500136e-31\n",
    "\n",
    "def add_target(info, target):\n",
    "    info['target'] = target_value\n",
    "    if target_value == 'NA' and 'cancer_target' in info:\n",
    "        info['target'] = info['cancer_target']  \n",
    "    elif target_value == 'NA' and 'category' in info:\n",
    "        if info['category'] == 'TP':\n",
    "            info['target'] = 1\n",
    "        elif info['category'] == 'TN':\n",
    "            info['target'] = 0 \n",
    "    return info\n",
    "\n",
    "def add_gnomad(info):\n",
    "    info['gnomad'] = 0\n",
    "    chrom, pos = info['hg19_chr'], info['hg19_pos(1-based)']\n",
    "    ref, alt = info['ref'], info['alt']\n",
    "    var_id = '_'.join([chrom, pos, ref, alt])\n",
    "    if var_id in gnomad_af:\n",
    "        info['gnomad']= gnomad_af[var_id]    \n",
    "    return info\n",
    "\n",
    "def add_secondary(info):\n",
    "    gene, aaref = info['genename'], info['aaref']\n",
    "    info['secondary_H'] = 0\n",
    "    info['secondary_C'] = 0\n",
    "    info['secondary_E'] = 0\n",
    "    if gene in secondary:\n",
    "        aapos = info['aapos'].split(';')\n",
    "        for pos in aapos:\n",
    "            pos = int(pos)\n",
    "            # AA_seq start from 0(it's a list)\n",
    "            protein_length = len(AA_seq[gene])\n",
    "            if pos < protein_length and AA_seq[gene][pos-1] == aaref:\n",
    "                if pos in secondary[gene]:\n",
    "                    if secondary[gene][pos] == 'H':\n",
    "                        info['secondary_H'] = 1\n",
    "                    elif secondary[gene][pos] == 'C':\n",
    "                        info['secondary_C'] = 1    \n",
    "                    elif secondary[gene][pos] == 'E':\n",
    "                        info['secondary_E'] = 1\n",
    "    return info\n",
    "\n",
    "def add_BioPlex(info):\n",
    "    ''' some feather related to protein? added all pint \n",
    "        http://bioplex.hms.harvard.edu/downloadInteractions.php\n",
    "    '''\n",
    "    gene = info['genename']\n",
    "    info['BioPlex'] = 0\n",
    "    if gene in BioPlex:\n",
    "        info['BioPlex'] = BioPlex[gene]\n",
    "    return info\n",
    "\n",
    "# SCORE = []\n",
    "# genesets = []\n",
    "# NA_genenum =0\n",
    "# with open ('../data/gene/genename_list.txt','rb') as fin:\n",
    "#     r = csv.reader(fin)\n",
    "#     for line in r: # line is a list! not a string\n",
    "#         #print line[0]\n",
    "#         genesets.append(line)\n",
    "# for a in genesets:\n",
    "#     if a[0] not in BioPlex.keys(): # a is a single-item list, not hashable, cannot used as dict keys\n",
    "# #        print a[0]\n",
    "# #        SCORE.append(BioPlex[a[0]])\n",
    "# #    else:\n",
    "#         NA_genenum += 1\n",
    "# print \"BioPlex\"\n",
    "# #print BioPlex\n",
    "# #print np.median(SCORE) #5.8844934185\n",
    "# #print np.mean(SCORE) #9.79657004979\n",
    "# #print np.min(SCORE) #0.750463933\n",
    "# print NA_genenum\n",
    "# print BioPlex['ADA']\n",
    "\n",
    "\n",
    "def add_REVEL(info, Tabix):\n",
    "    info['REVEL'] = -1\n",
    "    chrom, pos = info['hg19_chr'], info['hg19_pos(1-based)']\n",
    "    ref, alt = info['ref'], info['alt']\n",
    "    for row in Tabix.fetch(chrom, int(pos)-1, int(pos)+1):# 0-based inputin .fetch\n",
    "        row = row.split('\\t')\n",
    "        if row[3] == ref and row[4] == alt:\n",
    "            info['REVEL'] =  row[5]\n",
    "    return info  \n",
    "\n",
    "def add_MPC(info, Tabix):\n",
    "    info['MPC'] = -1\n",
    "    info['mis_badness'] = -1\n",
    "    info['obs_exp'] = -1\n",
    "    chrom, pos = info['hg19_chr'], info['hg19_pos(1-based)']\n",
    "    ref, alt = info['ref'], info['alt']\n",
    "    for row in Tabix.fetch(chrom, int(pos)-1, int(pos)+1):# 0-based inputin .fetch\n",
    "        row = row.split('\\t')\n",
    "        if row[2] == ref and row[3] == alt:\n",
    "            info['MPC'] = row[-1]\n",
    "            info['mis_badness'] = row[-3]\n",
    "            info['obs_exp'] = row[-4]\n",
    "    return info\n",
    "\n",
    "# SCORE = []\n",
    "# genesets = []\n",
    "# NA_genenum =0\n",
    "# with open ('../data/gene/genename_list.txt','rb') as fin:\n",
    "#     r = csv.reader(fin)\n",
    "#     for line in r: # line is a list! not a string\n",
    "#         #print line[0]\n",
    "#         genesets.append(line)\n",
    "# for a in genesets:\n",
    "#     if a[0] in prec.keys(): # a is a single-item list, not hashable, cannot used as dict keys\n",
    "#         SCORE.append(prec[a[0]])\n",
    "#     else:\n",
    "#         NA_genenum += 1\n",
    "# print \"prec\"\n",
    "# print np.median(SCORE) #0.518836940302\n",
    "# print np.mean(SCORE) #0.49179236685\n",
    "# print np.min(SCORE) #6.80216500136e-31\n",
    "\n",
    "\n",
    "\n",
    "def choose_HS(info):\n",
    "    include_variants = False  \n",
    "    if float(info['pli']) < 0.5 and float(info['pli']) >= 0: # HS genes\n",
    "        include_variants = True  \n",
    "    return include_variants\n",
    "\n",
    "def choose_HIS(info):\n",
    "    include_variants = False  \n",
    "    if float(info['pli']) >= 0.5: # HIS genes\n",
    "        include_variants = True  \n",
    "    return include_variants\n",
    "\n",
    "def choose_All(info):\n",
    "    include_variants = False  \n",
    "    if float(info['pli']) >= 0: # ALL genes\n",
    "        include_variants = True  \n",
    "    return include_variants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, ExAC_AF=0.01, write_head=True):\n",
    "    \"\"\" function that selected colums from wgsa, add some other feathers \n",
    "        set . into 0\n",
    "    \"\"\"\n",
    "    with open(fin, 'rU') as f:\n",
    "        positive, negative = 0, 0\n",
    "        \n",
    "        r = csv.reader(f)\n",
    "        head = r.next()\n",
    "        feat_all = wgsa_feat + add_feat + extra_feat\n",
    "        feat = []\n",
    "        for f in order_feat:\n",
    "            if f in feat_all:\n",
    "                feat.append(f)\n",
    "        for f in feat_all:\n",
    "            if f not in feat:\n",
    "                feat.append(f)\n",
    "        if write_head:\n",
    "            w.writerow(feat)\n",
    "\n",
    "        for line in r:\n",
    "            info = dict(zip(head, line))\n",
    "            aaref, aaalt, aapos = info['aaref'], info['aaalt'], info['aapos']                \n",
    "            \n",
    "            var_id = '_'.join([info['hg19_chr'], info['hg19_pos(1-based)'], info['ref'], info['alt']])\n",
    "            info['var_id'] = var_id\n",
    "            if var_id in exclude_var: continue\n",
    "\n",
    "            # exclude nonsense variants and syn\n",
    "            if aaref not in {'X', '.'} and aaalt not in [\".\", 'X']: \n",
    "                # reformat wsga feat, missing value filled with 0, will -1 be better?\n",
    "                for c in wgsa_feat:\n",
    "                    if info[c] == '.':\n",
    "                        if c in {'ExAC_AF', '1000Gp3_AF'}:\n",
    "                            info[c] = 0\n",
    "                        else:\n",
    "                            info[c] = -1\n",
    "                    else:\n",
    "                        info[c] = float(info[c])\n",
    "                        \n",
    "                # set some default value for extra_feat\n",
    "                for c in extra_feat:\n",
    "                    info[c] = info.get(c, -1)\n",
    "                \n",
    "                info['genename'] = info['genename']\n",
    "                info['blosum62'] = matrix_score(aaref, aaalt, 'blosum62') # function defined in previous cell\n",
    "                info['pam250'] = matrix_score(aaref, aaalt, 'pam250')\n",
    "                \n",
    "                # update SUMO/phospho scores\n",
    "                info['phospho_score'] = 0\n",
    "                info['phospho_cutoff'] = 0\n",
    "                info['phospho_diff'] = 0\n",
    "                gene = info['genename']\n",
    "                if gene in phosphorylation:\n",
    "                    aapos = info['aapos'].split(';')\n",
    "                    for pos in aapos:\n",
    "                        pos = int(pos)\n",
    "                        # pos is 1 based\n",
    "                        if pos in phosphorylation[gene]:\n",
    "                            if phosphorylation[gene][pos]['AA'] == aaref:\n",
    "                                info['phospho_score'] = phosphorylation[gene][pos]['Score']\n",
    "                                info['phospho_cutoff'] = phosphorylation[gene][pos]['Cutoff']\n",
    "                                info['phospho_diff'] = phosphorylation[gene][pos]['diff']\n",
    "                                break\n",
    "                                \n",
    "                info['SUMO_score'] = 0\n",
    "                info['SUMO_cutoff'] = 0\n",
    "                info['SUMO_diff'] = 0\n",
    "                if gene in SUMO:\n",
    "                    aapos = info['aapos'].split(';')\n",
    "                    for pos in aapos:\n",
    "                        pos = int(pos)\n",
    "                        # pos is 1 based\n",
    "                        if pos in SUMO[gene]:\n",
    "                            if SUMO[gene][pos]['AA'] == aaref:\n",
    "                                info['SUMO_score'] = SUMO[gene][pos]['Score']\n",
    "                                info['SUMO_cutoff'] = SUMO[gene][pos]['Cutoff']\n",
    "                                info['SUMO_diff'] = SUMO[gene][pos]['diff']\n",
    "                                break  \n",
    "                                \n",
    "                # add inteface flag\n",
    "                info['interface'] = 0\n",
    "                if gene in interface:\n",
    "                    aapos = info['aapos'].split(';')\n",
    "                    for pos in aapos:\n",
    "                        pos = int(pos)\n",
    "                        # AA_seq start from 0\n",
    "                        protein_length = len(AA_seq[gene])\n",
    "                        if pos < protein_length and AA_seq[gene][pos-1] == aaref:\n",
    "                            if pos in interface[gene]:\n",
    "                                info['interface'] = 1\n",
    "                                \n",
    "                # add ASA score Accessible Surface Areas\n",
    "                info['ASA'] = 0\n",
    "                if gene in ASA:\n",
    "                    aapos = info['aapos'].split(';')\n",
    "                    for pos in aapos:\n",
    "                        pos = int(pos)\n",
    "                        # AA_seq start from 0\n",
    "                        protein_length = len(AA_seq[gene])\n",
    "                        if pos < protein_length and AA_seq[gene][pos-1] == aaref:\n",
    "                            if pos in ASA[gene]:\n",
    "                                info['ASA'] = ASA[gene][pos]\n",
    "                \n",
    "\n",
    "                                    \n",
    "                info['ubiquitination'] = 0\n",
    "                if gene in ubiquitination:\n",
    "                    aapos = info['aapos'].split(';')\n",
    "                    for pos in aapos:\n",
    "                        pos = int(pos)\n",
    "                        # AA_seq start from 0\n",
    "                        protein_length = len(AA_seq[gene])\n",
    "                        if pos < protein_length and AA_seq[gene][pos-1] == aaref:\n",
    "                            if pos in ubiquitination[gene]:\n",
    "                                info['ubiquitination'] = ubiquitination[gene][pos]\n",
    "                \n",
    "                # gene specific feathers\n",
    "                info['complex_CORUM'] = 0\n",
    "                if gene in complex_CORUM:\n",
    "                    info['complex_CORUM'] = 1\n",
    "                \n",
    "                info['preppi_counts'] = 0\n",
    "                if gene in preppi:\n",
    "                    info['preppi_counts'] = preppi[gene]\n",
    "                    \n",
    "                info = add_secondary(info)\n",
    "                info = add_exac_metric(info)\n",
    "                info = add_gc_content(info)\n",
    "                info = add_s_het(info)\n",
    "                info = add_BioPlex(info)\n",
    "                info = add_MPC(info, f_MPC)\n",
    "                info = add_REVEL(info, f_revel)\n",
    "                \n",
    "                info = add_target(info, target_value)\n",
    "                info = add_gnomad(info)\n",
    "               \n",
    "                # choose variants in HIS or HS\n",
    "                if prefix == '.HS.':\n",
    "                    include_variants = choose_HS(info)\n",
    "                elif prefix == '.HIS.':\n",
    "                    include_variants = choose_HIS(info)\n",
    "                elif prefix == '.All.':\n",
    "                    include_variants = choose_All(info)\n",
    "                \n",
    "                # 201707016 remove variants with 0.1% in training/testing\n",
    "                if float(info['ExAC_AF']) > ExAC_AF:\n",
    "                    include_variants = False\n",
    "                    \n",
    "                    \n",
    "                if include_variants:\n",
    "                    if info['target'] == 1:\n",
    "                        positive += 1\n",
    "                    if info['target'] == 0:\n",
    "                        negative += 1\n",
    "                    \n",
    "                    w.writerow([info[c] for c in feat])\n",
    "                    \n",
    "    print '{} pos, {} neg'.format(positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variants excluded for training\n",
    "exclude_var = set()\n",
    "with open('../data/excluded_variants_gwas.txt') as f:\n",
    "    for line in f:\n",
    "        exclude_var.add(line.strip())  \n",
    "        \n",
    "with open('../data/input_data.exclude.txt') as f:\n",
    "    for line in f:\n",
    "        exclude_var.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load protein related annotations ## these are dictionaries !!! not array\n",
    "SUMO = np.load('../data/protein/SUMO.npy').item()\n",
    "phosphorylation = np.load('../data/protein/phosphorylation.npy').item()\n",
    "AA_seq = np.load('../data/protein/AA_seq.npy').item()\n",
    "interface = np.load('../data/protein/interface.npy').item()\n",
    "ASA = np.load('../data/protein/ASA.npy').item()\n",
    "preppi = np.load('../data/protein/preppi.npy').item()\n",
    "secondary = np.load('../data/protein/secondary.npy').item()\n",
    "ubiquitination = np.load('../data/protein/ubiquitination.npy').item()\n",
    "BioPlex = np.load('../data/protein/BioPlex.npy').item()\n",
    "\n",
    "s_het = np.load('../data/gene/s_het.npy').item()\n",
    "prec = np.load('../data/gene/prec.npy').item()\n",
    "pli = np.load('../data/gene/pli.npy').item()\n",
    "lofz = np.load('../data/gene/lofz.npy').item()\n",
    "\n",
    "gnomad_af = np.load('../data/training/gnomad_af.npy').item()\n",
    "\n",
    "\n",
    "complex_CORUM = set()\n",
    "with open('../data/protein/protein_complex_CORUM.txt') as f:\n",
    "    for line in f:\n",
    "        lst = line.strip().split('\\t')\n",
    "        complex_CORUM = complex_CORUM | set(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature order from correlation cluster\n",
    "order_feat = [u'MutationAssessor_score_rankscore', u'VEST3_rankscore', u'Polyphen2_HDIV_rankscore',\n",
    " u'Polyphen2_HVAR_rankscore', u'SIFT_converted_rankscore', u'PROVEAN_converted_rankscore',\n",
    " u'MetaSVM_rankscore',u'MetaLR_rankscore', u'FATHMM_converted_rankscore', u'M-CAP_rankscore',\n",
    " u'GenoCanyon_score_rankscore', u'LRT_converted_rankscore', u'Eigen-PC-raw_rankscore',\n",
    " u'Eigen-phred', u'Eigen-PC-phred', u'DANN_rankscore', u'CADD_phred', u'CADD_raw_rankscore',\n",
    " u'phyloP20way_mammalian_rankscore', u'GERP++_RS_rankscore', u'SiPhy_29way_logOdds_rankscore',\n",
    " u'phastCons100way_vertebrate_rankscore', u'fathmm-MKL_coding_rankscore', u'phyloP100way_vertebrate_rankscore',\n",
    " u'MutationTaster_converted_rankscore', u'phastCons20way_mammalian_rankscore', u'GM12878_fitCons_score_rankscore',\n",
    " u'HUVEC_fitCons_score_rankscore', u'integrated_fitCons_score_rankscore',u'H1-hESC_fitCons_score_rankscore', \n",
    " u'blosum62', u'pam250', u'SUMO_diff', u'SUMO_score', u'SUMO_cutoff', u'phospho_cutoff', u'phospho_score',\n",
    " u'phospho_diff', u'lofz', u'prec', u'pli',\n",
    " u's_het', u's_het_log', u'secondary_E', u'secondary_H', u'complex_CORUM', u'preppi_counts',\n",
    " u'1000Gp3_AF', u'ExAC_AF', 'gnomad', u'ASA', u'secondary_C', u'gc_content', u'interface', u'ubiquitination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add feathers from WGSA and other inputs, some of them need to be excluded in future training\n",
    "rank_score_cols = ['SIFT_converted_rankscore', 'Polyphen2_HDIV_rankscore', 'Polyphen2_HVAR_rankscore', \n",
    " 'LRT_converted_rankscore', 'MutationTaster_converted_rankscore', 'MutationAssessor_score_rankscore', \n",
    " 'FATHMM_converted_rankscore', 'PROVEAN_converted_rankscore', 'VEST3_rankscore', \n",
    " 'MetaSVM_rankscore', 'MetaLR_rankscore', 'M-CAP_rankscore', \n",
    " 'CADD_raw_rankscore', 'DANN_rankscore', 'fathmm-MKL_coding_rankscore', \n",
    " 'Eigen-PC-raw_rankscore', 'GenoCanyon_score_rankscore', 'integrated_fitCons_score_rankscore', \n",
    " 'GM12878_fitCons_score_rankscore', 'H1-hESC_fitCons_score_rankscore', \n",
    " 'HUVEC_fitCons_score_rankscore', 'GERP++_RS_rankscore', \n",
    " 'phyloP100way_vertebrate_rankscore', 'phyloP20way_mammalian_rankscore', \n",
    " 'phastCons100way_vertebrate_rankscore', 'phastCons20way_mammalian_rankscore', \n",
    " 'SiPhy_29way_logOdds_rankscore']\n",
    "\n",
    "wgsa_feat = ['1000Gp3_AF', 'ExAC_AF', 'CADD_phred', 'Eigen-phred', 'Eigen-PC-phred', 'RVIS']\n",
    "wgsa_feat = wgsa_feat + rank_score_cols\n",
    "\n",
    "add_feat =  ['blosum62', 'pam250', 'SUMO_score', 'SUMO_cutoff', 'SUMO_diff',\n",
    "             'phospho_score', 'phospho_cutoff','phospho_diff', 'interface',\n",
    "             'ASA', 'pli', 'lofz', 'complex_CORUM', 'preppi_counts',\n",
    "             'secondary_H', 'secondary_C', 'secondary_E', 'ubiquitination',\n",
    "             's_het', 'prec', 's_het_log',   'gc_content', 'gnomad', 'BioPlex',\n",
    "             'obs_exp', 'mis_badness', 'MPC', 'REVEL', \n",
    "             'target'] \n",
    "              #MPC and REVEL will be excluded cuz models.py self.excluded.features label them with x infront\n",
    "\n",
    "# feathers used for future info\n",
    "extra_feat = ['hg19_chr', 'hg19_pos(1-based)', \n",
    "              'ref', 'alt', 'category', 'source','INFO', 'disease', 'genename', 'var_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add annotation to training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9644 pos, 0 neg\n",
      "0 pos, 8371 neg\n",
      "5654 pos, 2704 neg\n",
      "0 pos, 7647 neg\n",
      "2367 pos, 0 neg\n"
     ]
    }
   ],
   "source": [
    "fout = '../data/input_data' + prefix + 'csv'\n",
    "\n",
    "with open(fout, 'w') as fw:\n",
    "    w = csv.writer(fw)\n",
    "    \n",
    "    # HGMD positive training\n",
    "    fin = '../data/training/HGMD_DM_missense_norecceive.rare.csv' \n",
    "    target_value = 1\n",
    "    sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, write_head=True)\n",
    "\n",
    "    # Discover negative data\n",
    "    fin = '../data/training/DiscovEHR_rare_missense_30000.csv' \n",
    "    target_value = 0\n",
    "    sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, write_head=False)\n",
    "    \n",
    "    # metaSVM training \n",
    "    fin = '../data/training/metaSVM_train.anno.rare.csv' \n",
    "    target_value = 'NA'\n",
    "    sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, write_head=False)\n",
    "    \n",
    "    # CADD negative data\n",
    "    fin = '../data/training/CADD_neg_train.anno.rare.csv' \n",
    "    target_value = 0\n",
    "    sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, write_head=False)\n",
    "\n",
    "    # clinvar\n",
    "    fin = '../data/training/clinvar_pathogenic_1-4star.rare.csv'\n",
    "    target_value = 1\n",
    "    sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, write_head=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Eigen-phred': {-1: 5.016389}, 'GenoCanyon_score_rankscore': {-1: 0.7471300000000001}, 'Polyphen2_HDIV_rankscore': {-1: 0.71542}, 'PROVEAN_converted_rankscore': {-1: 0.70749}, 'mis_badness': {-1: 0.500618690121}, 'M-CAP_rankscore': {-1: 0.92843}, 'VEST3_rankscore': {-1: 0.85854}, 'BioPlex': {-1: 6.933089557000001}, 'MPC': {-1: 1.17466397314}, 'CADD_phred': {-1: 25.5}, 'phyloP20way_mammalian_rankscore': {-1: 0.55125}, 'DANN_rankscore': {-1: 0.76315}, 'GERP++_RS_rankscore': {-1: 0.66886}, 'obs_exp': {-1: 0.702004939941}, 'GM12878_fitCons_score_rankscore': {-1: 0.59439}, 'MetaSVM_rankscore': {-1: 0.90964}, 'Eigen-PC-phred': {-1: 5.109498}, 'fathmm-MKL_coding_rankscore': {-1: 0.6867749999999999}, 'prec': {-1: 0.00655076650730545}, 'REVEL': {-1: 0.718}, 'ASA': {-1: 18.7}, 'MutationAssessor_score_rankscore': {-1: 0.6997399999999999}, 'Eigen-PC-raw_rankscore': {-1: 0.67629}, 'SiPhy_29way_logOdds_rankscore': {-1: 0.6905100000000001}, 'lofz': {-1: 4.54361060310315}, 'Polyphen2_HVAR_rankscore': {-1: 0.71516}, 'CADD_raw_rankscore': {-1: 0.69771}, 'phastCons100way_vertebrate_rankscore': {-1: 0.71511}, 'integrated_fitCons_score_rankscore': {-1: 0.60946}, 'complex_CORUM': {-1: 0.0}, 's_het': {-1: 0.114129508}, 'SIFT_converted_rankscore': {-1: 0.72092}, 'LRT_converted_rankscore': {-1: 0.84324}, 'preppi_counts': {-1: 88.0}, 's_het_log': {-1: 0.10807338968517413}, 'phastCons20way_mammalian_rankscore': {-1: 0.65342}, 'pli': {-1: 0.9934490233767609}, 'ubiquitination': {-1: 0.0}, 'phyloP100way_vertebrate_rankscore': {-1: 0.72726}, 'HUVEC_fitCons_score_rankscore': {-1: 0.6343}, 'FATHMM_converted_rankscore': {-1: 0.9046299999999999}, 'RVIS': {-1: -0.5482083839999999}, 'H1-hESC_fitCons_score_rankscore': {-1: 0.64085}, 'MetaLR_rankscore': {-1: 0.911125}, 'gc_content': {-1: 0.6}, 'MutationTaster_converted_rankscore': {-1: 0.81033}}\n"
     ]
    }
   ],
   "source": [
    "# get median dict of columns non -1 value \n",
    "cols_fill_with_neg = ['MutationAssessor_score_rankscore',\n",
    " 'VEST3_rankscore', 'Polyphen2_HDIV_rankscore', 'Polyphen2_HVAR_rankscore',\n",
    " 'SIFT_converted_rankscore', 'PROVEAN_converted_rankscore',\n",
    " 'MetaSVM_rankscore', 'MetaLR_rankscore', 'FATHMM_converted_rankscore',\n",
    " 'M-CAP_rankscore', 'GenoCanyon_score_rankscore', 'LRT_converted_rankscore', 'Eigen-PC-raw_rankscore',\n",
    " 'Eigen-phred', 'Eigen-PC-phred', 'DANN_rankscore',  'CADD_phred', 'CADD_raw_rankscore',\n",
    " 'phyloP20way_mammalian_rankscore', 'GERP++_RS_rankscore', 'SiPhy_29way_logOdds_rankscore',\n",
    " 'phastCons100way_vertebrate_rankscore', 'fathmm-MKL_coding_rankscore',\n",
    " 'phyloP100way_vertebrate_rankscore', 'MutationTaster_converted_rankscore',\n",
    " 'phastCons20way_mammalian_rankscore', 'GM12878_fitCons_score_rankscore',\n",
    " 'HUVEC_fitCons_score_rankscore', 'integrated_fitCons_score_rankscore',\n",
    " 'H1-hESC_fitCons_score_rankscore', 'lofz', 'prec', 'pli', 's_het',\n",
    " 's_het_log', 'complex_CORUM', 'preppi_counts', 'ASA', 'gc_content', 'ubiquitination',\n",
    " 'RVIS', 'BioPlex', 'obs_exp', 'mis_badness', 'MPC', 'REVEL']\n",
    "\n",
    "fout = '../data/input_data' + prefix + 'csv'\n",
    "median_dict = {}\n",
    "df = pd.read_csv(fout)\n",
    "for col in cols_fill_with_neg:\n",
    "    median_dict[col] = {-1 : df[df[col] != -1][col].median()}\n",
    "\n",
    "print median_dict\n",
    "\n",
    "for i in median_dict.keys():\n",
    "    if len(median_dict[i])>1:\n",
    "        print median_dict[i]\n",
    "\n",
    "def replace_missing(fin, median_dict):\n",
    "    '''\n",
    "    fill missing value of -1 from median dict \n",
    "    Args:\n",
    "        fin (str): input filename\n",
    "        median_dict (dict): dict of missing value, {'col':'-1':col_median}\n",
    "    '''\n",
    "    df = pd.read_csv(fin)\n",
    "    df.replace(median_dict,  inplace=True)\n",
    "    df.to_csv(fin, index=False)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fout = '../data/input_data' + prefix + 'csv'\n",
    "replace_missing(fout, median_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add annotation to Testing sets: need to run all prefix seperately \n",
    "# add annotation to target NA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5654 pos, 2704 neg\n",
      "63 pos, 17 neg\n",
      "2002 pos, 2710 neg\n",
      "0 pos, 2083 neg\n",
      "8431 pos, 3062 neg\n",
      "4718 pos, 2513 neg\n",
      "0 pos, 0 neg\n",
      "0 pos, 0 neg\n",
      "3351 pos, 1547 neg\n",
      "2964 pos, 770 neg\n",
      "0 pos, 0 neg\n",
      "5654 pos, 2704 neg\n",
      "0 pos, 0 neg\n"
     ]
    }
   ],
   "source": [
    "# metaSVM and other test files\n",
    "fins = ['../data/metaSVM/metaSVM_train.anno.rare.csv', '../data/metaSVM/metaSVM_test1.anno.rare.csv', \n",
    "       '../data/metaSVM/metaSVM_test2.anno.rare.csv', '../data/metaSVM/metaSVM_test3.anno.rare.csv', \n",
    "       '../data/metaSVM/metaSVM_addtest1.anno.rare.csv', '../data/metaSVM/metaSVM_addtest2.anno.rare.csv',\n",
    "       '../data/cancer_hotspots/cancer_sel.csv',\n",
    "       '../data/gene_test/MCAP_test.anno.rare.csv',\n",
    "       '../data/paper_test/ClinVar.anno.rare.csv', '../data/paper_test/UniFun.anno.rare.csv',\n",
    "       '../data/training/DiscovEHR_rare_missense_30000.csv',\n",
    "       '../data/training/metaSVM_train.anno.rare.csv',\n",
    "       '../data/training/CADD_neg_train.anno.rare.csv']\n",
    "\n",
    "fouts = [f.split('.csv')[0] + prefix + 'reformat.csv' for f in fins]\n",
    "\n",
    "for fin, fout in zip(fins, fouts):\n",
    "    with open(fout, 'w') as fw:\n",
    "        w = csv.writer(fw)\n",
    "        target_value = 'NA'\n",
    "        sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, write_head=True)\n",
    "        replace_missing(fout, median_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add annotation to target 0 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 pos, 177 neg\n",
      "0 pos, 331 neg\n",
      "0 pos, 360 neg\n",
      "0 pos, 364 neg\n"
     ]
    }
   ],
   "source": [
    "# files with 0 as target, mainly de novo control\n",
    "fins = ['../data/case_control/control_900.anno.rare.csv',\n",
    "        '../data/case_control/control_1911.anno.rare.csv',\n",
    "        '../data/case_control/ssc_yale.anno.rare.csv',\n",
    "        '../data/case_control/control_MarkDaly.anno.rare.csv']\n",
    "\n",
    "fouts = [f.split('.csv')[0] + prefix + 'reformat.csv' for f in fins]\n",
    "\n",
    "for fin, fout in zip(fins, fouts):\n",
    "    with open(fout, 'w') as fw:\n",
    "        w = csv.writer(fw)\n",
    "        target_value = 0\n",
    "        sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, \n",
    "                        ExAC_AF=1.0/10**5,\n",
    "                        write_head=True)\n",
    "        replace_missing(fout, median_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add annotation to target 1 dataset, mainly denovo cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2163 pos, 0 neg\n",
      "1557 pos, 0 neg\n",
      "630 pos, 0 neg\n",
      "2083 pos, 0 neg\n",
      "100 pos, 0 neg\n",
      "8866 pos, 0 neg\n"
     ]
    }
   ],
   "source": [
    "fins = ['../data/case_control/case.anno.rare.csv', \n",
    "        '../data/case_control/DDD_new_0.2.anno.rare.csv',\n",
    "        '../data/case_control/chd_yale.anno.rare.csv',\n",
    "        '../data/case_control/case_MarkDaly.anno.rare.csv',\n",
    "        '../data/case_control/CDH_mis.rare.csv',\n",
    "        '../data/training/HGMD_DM_missense_norecceive.rare.csv']\n",
    "fouts = [f.split('.csv')[0] + prefix + 'reformat.csv' for f in fins]\n",
    "for fin, fout in zip(fins, fouts):\n",
    "    with open(fout, 'w') as fw:\n",
    "        w = csv.writer(fw)\n",
    "        target_value = 1\n",
    "        sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, \n",
    "                        ExAC_AF=1.0/10**5,\n",
    "                        write_head=True)\n",
    "        replace_missing(fout, median_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process all missense and all cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # all cancer hostspot and other files\n",
    "# fins = ['/data/hq2130/large_files/cancer_all.csv']\n",
    "\n",
    "# fouts = []\n",
    "# for f in fins:\n",
    "#     fouts.append(f.split('.csv')[0] + prefix + 'reformat.GCcorrected.csv')\n",
    "# for fin, fout in zip(fins, fouts):\n",
    "#     with open(fout, 'w') as fw:\n",
    "#         w = csv.writer(fw)\n",
    "#         target_value = 'NA'\n",
    "#         sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, write_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # all cancer hostspot and other files\n",
    "\n",
    "# prefix = '.All.'\n",
    "# fins = ['/data/hq2130/large_files/rare_missense_id.anno.rare.csv']\n",
    "\n",
    "# fouts = []\n",
    "# for f in fins:\n",
    "#     fouts.append(f.split('.csv')[0] + prefix + 'reformat.csv')\n",
    "# for fin, fout in zip(fins, fouts):\n",
    "#     with open(fout, 'w') as fw:\n",
    "#         w = csv.writer(fw)\n",
    "#         target_value = 'NA'\n",
    "#         sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, \n",
    "#                         ExAC_AF=1.0/10**2,\n",
    "#                         write_head=True)\n",
    "\n",
    "# print \"done\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add annotation for different training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prefix = '.HIS.'\n",
    "# prefix = '.HS.'\n",
    "\n",
    "\n",
    "# fin = '../data/training/HGMD_DM_missense_norecceive.rare.csv'\n",
    "# outname = fin.split('.csv')[0] + prefix + 'csv'\n",
    "# print outname\n",
    "\n",
    "# with open(outname, 'w') as fw:\n",
    "#     w = csv.writer(fw)\n",
    "\n",
    "#     # HGMD positive training\n",
    "#     #fin = '../data/training/HGMD_DM_missense_anno.rare.csv' \n",
    "#     #HGMD positive training ## positive\n",
    "     \n",
    "#     target_value = 1\n",
    "#     sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, \n",
    "#                     target_value, ExAC_AF=0.01,\n",
    "#                     write_head=True)\n",
    "# fw.close()\n",
    "\n",
    "# ###\n",
    "# fin = '../data/training/metaSVM_train.anno.rare.csv' \n",
    "# outname = fin.split('.csv')[0] + prefix + 'csv'\n",
    "# print outname\n",
    "\n",
    "# with open(outname, 'w') as fw:\n",
    "#     w = csv.writer(fw)\n",
    "    \n",
    "#     # metaSVM training ## uniprot positive\n",
    "    \n",
    "#     target_value = 'NA'\n",
    "#     sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, write_head=False)\n",
    "# fw.close()\n",
    "    \n",
    "# ###\n",
    "# fin = '../data/training/MPC_train.rare.csv' \n",
    "# outname = fin.split('.csv')[0] + prefix + 'csv'\n",
    "# print outname\n",
    "\n",
    "# with open(outname, 'w') as fw:\n",
    "#     w = csv.writer(fw)       \n",
    "#     # MPC train ## from Mark Daly paper 402 HIS positive\n",
    "#     target_value = 1\n",
    "#     sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, write_head=True)\n",
    "# fw.close()\n",
    "\n",
    "# ###\n",
    "# fin =  '../data/training/clinvar_pathogenic_1-4star.rare.csv' \n",
    "# outname = fin.split('.csv')[0] + prefix + 'csv'\n",
    "# print outname\n",
    "\n",
    "# with open(outname, 'w') as fw:\n",
    "#     w = csv.writer(fw)     \n",
    "#     # ClinVar from cc as training \n",
    "#     target_value = 1\n",
    "#     sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, write_head=False)\n",
    "# fw.close()\n",
    "\n",
    "# ###\n",
    "# fin = '../data/training/DiscovEHR_rare_missense_30000.csv' \n",
    "# outname = fin.split('.csv')[0] + prefix + 'csv'\n",
    "# print outname\n",
    "\n",
    "# with open(outname, 'w') as fw:\n",
    "#     w = csv.writer(fw)     \n",
    "#     # Discover negative data\n",
    "#     target_value = 0\n",
    "#     sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, write_head=False)\n",
    "# fw.close()\n",
    "\n",
    "\n",
    "# ###\n",
    "# fin = '../data/training/CADD_neg_train.anno.rare.csv' \n",
    "# outname = fin.split('.csv')[0] + prefix + 'csv'\n",
    "# print outname\n",
    "\n",
    "# with open(outname, 'w') as fw:\n",
    "#     w = csv.writer(fw)     \n",
    "# #CADD negative data\n",
    "# # #     \n",
    "#     target_value = 0\n",
    "#     sel_add_feather(fin, w, wgsa_feat, add_feat, extra_feat, target_value, write_head=False)\n",
    "# fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decide which training sets to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/input_data.HIS.csv\n",
      "['MutationAssessor_score_rankscore', 'VEST3_rankscore', 'Polyphen2_HDIV_rankscore', 'Polyphen2_HVAR_rankscore', 'SIFT_converted_rankscore', 'PROVEAN_converted_rankscore', 'MetaSVM_rankscore', 'MetaLR_rankscore', 'FATHMM_converted_rankscore', 'M-CAP_rankscore', 'GenoCanyon_score_rankscore', 'LRT_converted_rankscore', 'Eigen-PC-raw_rankscore', 'Eigen-phred', 'Eigen-PC-phred', 'DANN_rankscore', 'CADD_phred', 'CADD_raw_rankscore', 'phyloP20way_mammalian_rankscore', 'GERP++_RS_rankscore', 'SiPhy_29way_logOdds_rankscore', 'phastCons100way_vertebrate_rankscore', 'fathmm-MKL_coding_rankscore', 'phyloP100way_vertebrate_rankscore', 'MutationTaster_converted_rankscore', 'phastCons20way_mammalian_rankscore', 'GM12878_fitCons_score_rankscore', 'HUVEC_fitCons_score_rankscore', 'integrated_fitCons_score_rankscore', 'H1-hESC_fitCons_score_rankscore', 'blosum62', 'pam250', 'SUMO_diff', 'SUMO_score', 'SUMO_cutoff', 'phospho_cutoff', 'phospho_score', 'phospho_diff', 'lofz', 'prec', 'pli', 's_het', 's_het_log', 'secondary_E', 'secondary_H', 'complex_CORUM', 'preppi_counts', '1000Gp3_AF', 'ExAC_AF', 'gnomad', 'ASA', 'secondary_C', 'gc_content', 'interface', 'ubiquitination', 'RVIS', 'BioPlex', 'obs_exp', 'mis_badness', 'MPC', 'REVEL', 'target', 'hg19_chr', 'hg19_pos(1-based)', 'ref', 'alt', 'category', 'source', 'INFO', 'disease', 'genename', 'var_id']\n"
     ]
    }
   ],
   "source": [
    "prefix = '.HIS.'\n",
    "#prefix = '.HS.'\n",
    "fnames = ['../data/training/HGMD_DM_missense_norecceive.rare' + prefix + 'csv',\n",
    "         '../data/training/metaSVM_train.anno.rare' + prefix + 'csv',\n",
    "         #'../data/training/MPC_train.rare' + prefix + 'csv' , # 400 3&4 star clinvar\n",
    "         '../data/training/clinvar_pathogenic_1-4star.rare' + prefix + 'csv' ,\n",
    "         \n",
    "          '../data/training/DiscovEHR_rare_missense_30000' + prefix + 'csv' \n",
    "          #,'../data/training/CADD_neg_train.anno.rare.csv' \n",
    "         ]\n",
    "\n",
    "outname = '../data/input_data' + prefix + 'csv'\n",
    "print outname\n",
    "with open(outname, 'w') as fw:\n",
    "    w = csv.writer(fw) # create w as an object w for writing \n",
    "    flag= 0\n",
    "    for fname in fnames:\n",
    "        with open(fname,'rb') as f:\n",
    "            r = csv.reader(f) # create an object r for read\n",
    "            if flag == 0:\n",
    "                head = r.next() \n",
    "                print head\n",
    "                w.writerow(head) # write by rows\n",
    "                flag = 1\n",
    "            for line in r: # loop through r by rows\n",
    "                w.writerow(line)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feb  Jan account\n",
      "0    -1  100    chen\n",
      "1  1000   -1      HJ\n",
      "    Feb  Jan account\n",
      "0   500  100    chen\n",
      "1  1000   50      HJ\n"
     ]
    }
   ],
   "source": [
    "# # test\n",
    "# median_dict = {'account':{'.':'median'},'Jan':{-1:50},'Feb':{-1:500}}\n",
    "# #df = pd.read_csv(fout)\n",
    "# #for col in cols_fill_with_neg:\n",
    "# #    median_dict[col] = {-1 : df[df[col] != -1][col].median()}\n",
    "\n",
    "# def replace_missing(fin, median_dict):\n",
    "#     '''\n",
    "#     fill missing value of -1 from median dict \n",
    "#     Args:\n",
    "#         fin (str): input filename\n",
    "#         median_dict (dict): dict of missing value, {'col':'-1':col_median}\n",
    "#     '''\n",
    "#     df = pd.read_csv(fin)\n",
    "#     df.replace(median_dict,  inplace=True)\n",
    "#     df.to_csv(fin.split('.csv')[0]+'_output.csv', index=False)\n",
    "#     return \n",
    "\n",
    "# sales = [{'account': 'chen','Jan':100, 'Feb':-1},{'account':'HJ','Jan':-1, 'Feb':1000}]\n",
    "# df = pd.DataFrame(sales)#, index=['a', 'c', 'e', 'f', 'h'],columns=['one', 'two', 'three'])\n",
    "# print df\n",
    "# df.to_csv('../data/test.csv', index=False)\n",
    "# #print fin.split('.csv')[0]#+'_output.csv'\n",
    "# replace_missing('../data/test.csv',median_dict)\n",
    "\n",
    "\n",
    "# df = pd.read_csv('../data/test_output.csv')\n",
    "# print df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # def replace_missing(fin, median_dict):\n",
    "# #     '''\n",
    "# #     fill missing value of -1 from median dict \n",
    "# #     Args:\n",
    "# #         fin (str): input filename\n",
    "# #         median_dict (dict): dict of missing value, {'col':'-1':col_median}\n",
    "# #     '''\n",
    "# #     df = pd.read_csv(fin)\n",
    "# #     for index, row in df.iterrows():\n",
    "# #         if df.iloc[index,row]==-1:\n",
    "# #             df.iloc[index,row] = median_dict[index]\n",
    "        \n",
    "# #     #df.replace(median_dict,  inplace=True)\n",
    "# #     df.to_csv(fin, index=False)\n",
    "# #     return \n",
    "\n",
    "# # df.replace({-1:999},  inplace=True)\n",
    "# # print df\n",
    "# # print list(df)[0:1]\n",
    "\n",
    "# # for col in list(df)[0:1]:\n",
    "# #     median_dict[col] = {-1 : df[df[col] != -1][col].median()}\n",
    "# # df.replace({100:'nan'},  inplace=True)\n",
    "# # print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": false,
   "toc_section_display": "block",
   "toc_threshold": 6,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
